{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Definitions & setup logic\n",
    "# ────────────────────────────────────────────────────\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Compute project paths\n",
    "nb_dir   = Path.cwd().resolve()    # …/notebooks\n",
    "repo_dir = nb_dir.parent           # repo root\n",
    "\n",
    "# Make sure our repo root is on the import path *before* any pipeline imports\n",
    "sys.path.insert(0, str(repo_dir))\n",
    "\n",
    "# Now it will find pipeline.utils.duckdb_wrapper\n",
    "from pipeline.utils.duckdb_wrapper import DuckDBWrapper\n",
    "from pipeline.datasets import *\n",
    "\n",
    "# Helper: initialize a DuckDBWrapper with optional reset logic\n",
    "def initialize(duckdb_path: Path, delete_on_disk: bool, reset_catalog: bool):\n",
    "    \"\"\"\n",
    "    1) Closes any existing `con`\n",
    "    2) Deletes on‐disk files (duckdb + WAL) if delete_on_disk=True\n",
    "    3) Connects a fresh DuckDBWrapper at duckdb_path\n",
    "    4) Drops all tables/views in‐session if reset_catalog=True\n",
    "    Returns the new `con`.\n",
    "    \"\"\"\n",
    "    global con\n",
    "\n",
    "    # 1) close old connection\n",
    "    try:\n",
    "        con.con.close()\n",
    "        del con\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) delete on-disk files if asked\n",
    "    if delete_on_disk:\n",
    "        wal_file = duckdb_path.with_suffix(duckdb_path.suffix + \"-wal\")\n",
    "        for f in (duckdb_path, wal_file):\n",
    "            if f.exists():\n",
    "                f.unlink()\n",
    "\n",
    "    # 3) connect\n",
    "    duckdb_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    con = DuckDBWrapper(duckdb_path)\n",
    "\n",
    "    # 4) reset in-session catalog if asked\n",
    "    if reset_catalog:\n",
    "        rows = con.con.execute(\"\"\"\n",
    "            SELECT table_name, table_type\n",
    "            FROM information_schema.tables\n",
    "            WHERE table_schema = 'main'\n",
    "        \"\"\").fetchall()\n",
    "        for name, typ in rows:\n",
    "            con.con.execute(f'DROP {typ} IF EXISTS \"{name}\" CASCADE')\n",
    "\n",
    "    return con\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configure & initialize\n",
    "\n",
    "# 1) DuckDB file path (edit if you move it)\n",
    "duckdb_file  = Path(\"../data/duckdb/test.duckdb\")  \n",
    "\n",
    "# 2) One toggle for a “fresh start”\n",
    "FRESH_START  = True   # if True: deletes on‐disk + resets catalog; if False: leaves everything intact\n",
    "\n",
    "# 3) Run initialization (passes the same flag twice)\n",
    "con = initialize(\n",
    "    duckdb_path    = duckdb_file,\n",
    "    delete_on_disk = FRESH_START,\n",
    "    reset_catalog  = FRESH_START,\n",
    ")\n",
    "\n",
    "# 4) Inspect\n",
    "print(f\"Connected to: {duckdb_file}\")\n",
    "print(\"Current tables/views:\")\n",
    "con.show_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"data/opendata\"\n",
    "\n",
    "con.bulk_register_data(\n",
    "    repo_root   = repo_dir,\n",
    "    base_path   = BASE_PATH,\n",
    "    table_names = SINGLE_FILE_ASSETS_NAMES,\n",
    "    wildcard    = \"*.parquet\",\n",
    "    as_table    = False,\n",
    ")\n",
    "\n",
    "con.bulk_register_partitioned_data(\n",
    "    repo_root   = repo_dir,\n",
    "    base_path   = BASE_PATH,\n",
    "    table_names = PARTITIONED_ASSETS_NAMES,\n",
    "    wildcard    = \"year=*/month=*/*.parquet\",\n",
    "    as_table    = False,\n",
    "    show_tables = True,                 # optional: prints a neat summary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure export directory exists\n",
    "sql_dir    = nb_dir / \"sql\"                  # notebooks/sql/*.sql\n",
    "export_dir = repo_dir / \"data\" / \"exports\"   # where we write .parquet\n",
    "export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for sql_path in sorted(sql_dir.glob(\"*.sql\")):\n",
    "    sql_text  = sql_path.read_text()\n",
    "    out_name  = sql_path.stem                # e.g. rides, new, test\n",
    "    print(f\"▶︎ Running {sql_path.name}\")\n",
    "\n",
    "    result_df = con.run_query(sql_text)      # Polars DataFrame\n",
    "\n",
    "    con.export(\n",
    "        result     = result_df,\n",
    "        file_type  = \"parquet\",\n",
    "        base_path  = export_dir,\n",
    "        file_name  = out_name,\n",
    "    )\n",
    "\n",
    "print(f\"✓ Done — results saved under {export_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_paths  = sorted(export_dir.glob(\"*.parquet\"))\n",
    "view_names    = [p.stem for p in result_paths]          # \"new\", \"rides\", \"test\", …\n",
    "\n",
    "con.register_data_view(\n",
    "    paths        = result_paths,\n",
    "    table_names  = view_names\n",
    ")\n",
    "\n",
    "# optional: confirm they’re there\n",
    "con.show_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# Convert every notebooks/sql/*.sql into dbt‑compatible models\n",
    "# ---------------------------------------------------------------------\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "DEBUG = False                  # set True to enable convertdbtsql.py \"debug\" mode\n",
    "\n",
    "# 1) locate repo root\n",
    "nb_dir   = Path.cwd().resolve()           # .../notebooks\n",
    "repo_dir = nb_dir.parent                  # repo root\n",
    "\n",
    "# 2) build all required paths\n",
    "sql_dir   = nb_dir / \"sql\"                # source folder (already exists)\n",
    "dest_dir  = repo_dir / \"transformations\" / \"dbt\" / \"models\"\n",
    "tool_path = repo_dir / \"tools\" / \"convertdbtsql.py\"\n",
    "\n",
    "# 3) sanity checks\n",
    "for p in (sql_dir, tool_path):\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(p)\n",
    "\n",
    "dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 4) assemble argv and run\n",
    "argv = [\n",
    "    sys.executable,                       # the current Python interpreter\n",
    "    str(tool_path),\n",
    "    str(dest_dir),\n",
    "    str(sql_dir),\n",
    "]\n",
    "if DEBUG:\n",
    "    argv.append(\"debug\")\n",
    "\n",
    "print(\"▶︎ Converting SQL files for dbt …\")\n",
    "subprocess.run(argv, check=True)\n",
    "print(f\"✓ Done — rewritten models are in {dest_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
